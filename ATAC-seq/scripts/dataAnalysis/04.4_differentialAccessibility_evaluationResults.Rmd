---
title: "Differential accessibility analysis - comparison of methods"
date: "12 April 2019"
output:
  html_document:
    keep_md: true
    fig_width: 5
    fig_height: 5
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_float: 
      collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(csaw)
library(edgeR)
library(ChIPpeakAnno)
library(ggplot2)

dir <- "/user01/group_folders/Personal/Ximena/SOMITES/somitogenesis2019/"
```


We have performed differential accessibility analyses testing differences between the somite trios and across developmental stages. The data has been normalised to account for trended efficiency biases. We have used different approaches, 

1. **Baseline**: DA testing directly of the normalised data.
2. **SVA approach**: testing with an added surrogate variable (SV) that captures observed residual effects of efficiency biases.
3. **PCA approach**: testing with added 18 PCs from running PCA on the residuals of the model fit with the biological design of interest.

A PCA of the normalised data shows strong correlation of the first PCs with the fraction of reads in peaks (FRiP), suggesting the normalisation hasn't completely removed technical biases. After regressing out the inferred SV, PCs no longer correlate with FRiP and segregate by stage a little bit better. If instead the 18 PCs are regressed out, samples cluster clearly by stage. This suggests that there are residual technical effects in the data and capturing them through PCA on the residuals produces a much cleaner dataset, with obvious differences between the biological groups of interest.

So let's compare the results obtained with each approach. We take the results from using averaged values (across stages for somite comparisons, and across somites for stages comparisons), to make things easier.

## Somite trios

When we compare the somite trios, most differences are detected between the first and last somites, which are the most distinct. When comparing adjacent somites the differences are too subtle to be significant.

Regressing out the PCs from the residuals of the fit increases power substantially, and many more regions are recovered. Regressing out the SV instead reduces the number of significant regions; this could mean that some regions recovered when using only the normalised data are false positives due to the residual technical biases.

```{r trios_ave}
trios.norm <- readRDS(paste0(dir, "ATAC-seq/results/04.1_diffAccessibility_somiteTrios_average.Rds"))
trios.sva <- readRDS(paste0(dir, "ATAC-seq/results/04.2_diffAccessibility_somiteTrios_average_sva.Rds"))
trios.pca <- readRDS(paste0(dir, "ATAC-seq/results/04.3_diffAccessibility_somiteTrios_average_pca.Rds"))

trios.norm <- sapply(1:3, function(x) trios.norm[[x]] <- trios.norm[[x]][trios.norm[[x]]$FDR<0.05,])
trios.sva <- sapply(1:3, function(x) trios.sva[[x]] <- trios.sva[[x]][trios.sva[[x]]$FDR<0.05,])
trios.pca <- sapply(1:3, function(x) trios.pca[[x]] <- trios.pca[[x]][trios.pca[[x]]$FDR<0.05,])

nDE <- t(data.frame(baseline=unlist(lapply(trios.norm, length)), SVA=unlist(lapply(trios.sva, length)), PCA=unlist(lapply(trios.pca, length))))
colnames(nDE) <- c("somiteIvsII", "somiteIIIvsI", "somiteIIIvsII")
nDE
```

Given the numbers, let's just compare the results for the somite I vs III comparison. Most of the regions significant after controlling for the SV are also significant when regressing out the PCs. When using only the normalised data, a large fraction of the regions overlap with those detected with the `sva` and `pca` approaches, but still a significant fraction (`r round(178/length(trios.norm[[2]])*100,2)`%) are not reproduced.  

```{r overlap, warning=FALSE, message=FALSE}
overlap <- makeVennDiagram(list(base=trios.norm[[2]], SVA=trios.sva[[2]], PCA=trios.pca[[2]]), fill=c("tomato1", "steelblue", "darkolivegreen2"), alpha=0.25, cat.col=c("tomato1", "steelblue", "darkolivegreen4"), cat.fontface=2)
```

The regions that are not reproduced in the other methods tend to have much higher FDRs, suggesting they are not as robust.

```{r normOnly, message=FALSE}
norm_only <- trios.norm[[2]][trios.norm[[2]] %outside% unique(c(trios.sva[[2]], trios.pca[[2]]))]
shared <- trios.norm[[2]][trios.norm[[2]] %outside% norm_only]

df <- data.frame(FDR = c(shared$FDR, norm_only$FDR), baseline_regions=c(rep("shared", length(shared)), rep("unique", length(norm_only))))
ggplot(df, aes(baseline_regions, FDR)) + geom_violin() + geom_boxplot(width=0.075)
```

So, could these be false positives?

We count the number of fragments mapped to the regions we defined as peaks through the `csaw` workflow, and normalise their abundance based on trended normalisation, similar to what we did with the windows.

```{r peakCounts, message=FALSE}
## metadata
meta <- read.table(paste0(dir, "ATAC-seq/data/metadata_ATACseq_GQ.tsv"), stringsAsFactors = FALSE, header = TRUE)

## peak set
filtered.data <- readRDS(file=paste0(dir, "ATAC-seq/results/03_windowCounts_filteredWindows.Rds"))
merged <- mergeWindows(rowRanges(filtered.data), tol=150, max.width = 1500)

## count reads in peaks
blacklist <- readRDS(paste0(dir, "ATAC-seq/data/mm10-blacklist.v2.Rds"))
param <- readParam(discard=blacklist, restrict=paste0("chr", c(1:19, "X")), pe="both", dedup=FALSE, BPPARAM=MulticoreParam(workers=24))
peakCounts <- regionCounts(filtered.data$bam.files, merged$region, param = param)

peakCounts <- normOffsets(peakCounts, type="loess", se.out=TRUE)
sf.trended <- assay(peakCounts, "offset")
saveRDS(peakCounts, paste0(dir, "ATAC-seq/results/04.4_peakCounts_csawMerged.Rds"))

# raw counts
adjc <- log2(assay(peakCounts)+1)
# normalised
re.adjc <- adjc - sf.trended/log(2)

# abval <- aveLogCPM(asDGEList(peakCounts))
# o <- order(abval)
# par(mfrow=c(1,2))
# i=45
# mval <- adjc[,1]-adjc[,i]
# fit <- loessFit(x=abval, y=mval)
# smoothScatter(abval, mval, ylab="M", xlab="Average logCPM", main=paste("Raw 1 vs",i)) #, ylim=c(-2,2), xlim=c(0, 7))
# lines(abval[o], fit$fitted[o], col="red")
# 
# mval <- re.adjc[,1]-re.adjc[,i]
# fit <- loessFit(x=abval, y=mval)
# smoothScatter(abval, mval, ylab="M", xlab="Average logCPM", main=paste("Normalised 1 vs",i)) #, ylim=c(-2,2), xlim=c(0, 7))
# lines(abval[o], fit$fitted[o], col="red")
```

Looking at the correlation with FRiP of the counts from regions that are significant only when no covariates are included in the model, it is significantly higher than the correlation of regions that are significant in the other methods. This supports, to an extent, that these regions might be the result of the lack of correction for the residual FRiP effects.

```{r cor.frip}
row.names(re.adjc) <- paste0(as.data.frame(rowRanges(peakCounts))[,1], ":", as.data.frame(rowRanges(peakCounts))[,2], "-",as.data.frame(rowRanges(peakCounts))[,3])
colnames(re.adjc) <- meta$sample

tmp <- re.adjc[paste0(as.data.frame(norm_only)[,1], ":", as.data.frame(norm_only)[,2], "-", as.data.frame(norm_only)[,3]),]
cor.frip.unique <- sapply(1:nrow(tmp), function(x) cor(tmp[x,], meta$readsInPeakSet/meta$goodQuality*100))

tmp <- re.adjc[paste0(as.data.frame(shared)[,1], ":", as.data.frame(shared)[,2], "-", as.data.frame(shared)[,3]),]
cor.frip.shared <- sapply(1:nrow(tmp), function(x) cor(tmp[x,], meta$readsInPeakSet/meta$goodQuality*100))

boxplot(abs(cor.frip.shared), abs(cor.frip.unique), names=c("shared", "baseline-only"), ylab="Pearson r against FRiP")
# wilcox.test(abs(cor.frip.shared), abs(cor.frip.unique), alternative = "less") # p-value = 2.09e-13
```

What about the regions that are gained when we use the PCA-based approach?

It is also true that these regions tend to have higher FDRs, and thus are less robust.

```{r pcaOnly, message=FALSE}
pca_only <- trios.pca[[2]][trios.pca[[2]] %outside% unique(c(trios.sva[[2]], trios.norm[[2]]))]
shared <- trios.pca[[2]][trios.pca[[2]] %outside% pca_only]

df <- data.frame(FDR = c(shared$FDR, pca_only$FDR), PCA_regions=c(rep("shared", length(shared)), rep("unique", length(pca_only))))
ggplot(df, aes(PCA_regions, FDR)) + geom_violin() + geom_boxplot(width=0.075)
```

However, there is no difference in the correlation to FRiP between the regions that are shared and those gained in the PCA approach.

```{r cor.frip2}
tmp <- re.adjc[paste0(as.data.frame(pca_only)[,1], ":", as.data.frame(pca_only)[,2], "-", as.data.frame(pca_only)[,3]),]
cor.frip.unique <- sapply(1:nrow(tmp), function(x) cor(tmp[x,], meta$readsInPeakSet/meta$goodQuality*100))

tmp <- re.adjc[paste0(as.data.frame(shared)[,1], ":", as.data.frame(shared)[,2], "-", as.data.frame(shared)[,3]),]
cor.frip.shared <- sapply(1:nrow(tmp), function(x) cor(tmp[x,], meta$readsInPeakSet/meta$goodQuality*100))

boxplot(abs(cor.frip.shared), abs(cor.frip.unique), names=c("shared", "PCA-only"), ylab="Pearson r against FRiP")
# wilcox.test(abs(cor.frip.shared), abs(cor.frip.unique), alternative = "less") # p-value = 1
```

Although this suggests that the gained regions are not due to technical effects and that the PCA-approach has been successful in accounting for uninteresting variation and has increased power greatly. However, with such increase in power there is also the risk of identifying very small differences that are not biologically meaningful, and thus it is important to examine the fold-change of the significant regions. Since the analysis is performed at the window level and these are then merged into regions, it is not straight forward to compute a fold-change. But we can take the most significantly differential window's fold-change as a proxy.

Generally, most regions have a reasonable fold-change.

```{r fold-change.trios}
df <- data.frame(fc=c(shared$logFC, pca_only$logFC), shared=c(rep("yes",length(shared)), rep("no", length(pca_only))))
df$shared <- relevel(df$shared, "yes")
ggplot(df, aes(shared, fc)) + geom_violin() + geom_hline(yintercept = c(-log2(1.5), log2(1.5)), lty=2) + ylab("log2 fold-change")
#+ geom_hline(yintercept = c(median(df[df$shared=="yes" & df$fc>0,1]), median(df[df$shared=="yes" & df$fc<0,1]), median(df[df$shared=="no" & df$fc>0,1]), median(df[df$shared=="no" & df$fc<0,1])))

# sum(abs(df$fc) > log2(1.5))/nrow(df)*100 # 90.3% (1329 regions)
```

The majority (`r round(sum(abs(df$fc) > log2(1.5))/nrow(df)*100,2)`%) have an absolute fold-change of 1.5 or higher. Thus, it seems like we are not overpowered in detecting regions that are not meaningful, but we can still filter out those regions with fold-changes smaller than 1.5.

### Stage differences

In the case of regions changing between any pairwise stage comparison, we get similar results, with the SVA approach yielding the fewest significant regions and the PCA approach increasing the number significantly.

```{r stage_ave}
stage.norm <- readRDS(paste0(dir, "ATAC-seq/results/04.1_diffAccessibility_stages_average.Rds"))
stage.sva <- readRDS(paste0(dir, "ATAC-seq/results/04.2_diffAccessibility_stages_average_sva.Rds"))
stage.pca <- readRDS(paste0(dir, "ATAC-seq/results/04.3_diffAccessibility_stages_average_pca.Rds"))

stage.norm <- stage.norm[stage.norm$FDR<0.05,]
stage.sva <- stage.sva[stage.sva$FDR<0.05,]
stage.pca <- stage.pca[stage.pca$FDR<0.05,]

nDE <- t(data.frame(norm=length(stage.norm), SVA=length(stage.sva), PCA=length(stage.pca)))
colnames(nDE) <- "nDA"
nDE
```

```{r overlap.stage, warning=FALSE, message=FALSE}
overlap <- makeVennDiagram(list(NORM=stage.norm, SVA=stage.sva, PCA=stage.pca), fill=c("tomato1", "steelblue", "darkolivegreen2"), alpha=0.25, cat.col=c("tomato1", "steelblue", "darkolivegreen4"), cat.fontface=2)
```

For regions called only in the `norm` approach, the p-values are quite high.

```{r normOnly.stage, message=FALSE}
norm_only <- stage.norm[stage.norm %outside% unique(c(stage.sva, stage.pca))]
shared <- stage.norm[stage.norm %outside% norm_only]

df <- data.frame(FDR = c(shared$FDR, norm_only$FDR), norm_regions=c(rep("shared", length(shared)), rep("unique", length(norm_only))))
ggplot(df, aes(norm_regions, FDR)) + geom_violin() + geom_boxplot(width=0.075)
```

The difference in correlation values with FRiP is not as obvious but still significant.

```{r cor.frip.stage}
tmp <- re.adjc[paste0(as.data.frame(norm_only)[,1], ":", as.data.frame(norm_only)[,2], "-", as.data.frame(norm_only)[,3]),]
cor.frip.unique <- sapply(1:nrow(tmp), function(x) cor(tmp[x,], meta$readsInPeakSet/meta$goodQuality*100))

tmp <- re.adjc[paste0(as.data.frame(shared)[,1], ":", as.data.frame(shared)[,2], "-", as.data.frame(shared)[,3]),]
cor.frip.shared <- sapply(1:nrow(tmp), function(x) cor(tmp[x,], meta$readsInPeakSet/meta$goodQuality*100))

boxplot(abs(cor.frip.shared), abs(cor.frip.unique), names=c("shared", "norm-only"), ylab="Pearson r agains FRiP")
# wilcox.test(abs(cor.frip.shared), abs(cor.frip.unique), alternative = "less") # p-value = 4.917e-10
```

And the same for the regions gained with the PCA approach. Higher FDRs

```{r pcaOnly.stage, message=FALSE}
pca_only <- stage.pca[stage.pca %outside% unique(c(stage.sva, stage.norm))]
shared <- stage.pca[stage.pca %outside% pca_only]

df <- data.frame(FDR = c(shared$FDR, pca_only$FDR), norm_regions=c(rep("shared", length(shared)), rep("unique", length(pca_only))))
ggplot(df, aes(norm_regions, FDR)) + geom_violin() + geom_boxplot(width=0.075)
```

but no difference in correlation values whatsoever.

```{r cor.frip.stage2}
tmp <- re.adjc[paste0(as.data.frame(pca_only)[,1], ":", as.data.frame(pca_only)[,2], "-", as.data.frame(pca_only)[,3]),]
cor.frip.unique <- sapply(1:nrow(tmp), function(x) cor(tmp[x,], meta$readsInPeakSet/meta$goodQuality*100))

tmp <- re.adjc[paste0(as.data.frame(shared)[,1], ":", as.data.frame(shared)[,2], "-", as.data.frame(shared)[,3]),]
cor.frip.shared <- sapply(1:nrow(tmp), function(x) cor(tmp[x,], meta$readsInPeakSet/meta$goodQuality*100))

boxplot(abs(cor.frip.shared), abs(cor.frip.unique), names=c("shared", "norm-only"), ylab="Pearson r agains FRiP")
# wilcox.test(abs(cor.frip.shared), abs(cor.frip.unique), alternative = "less") # p-value = 1
```

And in this case the fold-changes are again quite large for most regions.

```{r fold-change.stage}
tmp <- as.data.frame(mcols(shared)[,35:49])
s <- apply(tmp, 1, function(x) x[which.max(abs(x))])
tmp <- as.data.frame(mcols(pca_only)[,35:49])
p <- apply(tmp, 1, function(x) x[which.max(abs(x))])

df <- data.frame(fc=c(s, p), shared=c(rep("yes",length(shared)), rep("no", length(pca_only))))
df$shared <- relevel(df$shared, "yes")
ggplot(df, aes(shared, fc)) + geom_violin() + geom_hline(yintercept = c(-log2(1.5), log2(1.5)), lty=2) + ylab("log2 fold-change")

# sum(abs(df$fc) > log2(1.5))/nrow(df)*100 # 97.3% (1329 regions)
```

Almost all regions have a fold-change of at least 1.5 (`r round(sum(abs(df$fc) > log2(1.5))/nrow(df)*100,2)`%) and even (`r round(sum(abs(df$fc) > log2(2))/nrow(df)*100,2)`%) are above 2.

Based on this, it seems like the approach using only the normalised data without covariates returns a few false positives. But more importantly, the approach with the PCA covariates increases power greatly, retrieving many more significant regions. The gained regions seem to be true positives, so we will continue with this set of DA results in downstream analyses.


```{r info}
sessionInfo()
```

